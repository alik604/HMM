{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hidden Markov Model in python - learn prams from data \n",
    "-----------------\n",
    "[Theory explanation: Hidden Markov Models - Bert Huang](https://www.youtube.com/watch?v=9yl4XGp5OEg) || \n",
    "[More scary Code - PyTorch](https://colab.research.google.com/drive/1IUe9lfoIiQsL49atSOgxnCmMR_zJazKI) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import mnist\n",
    "%matplotlib inline\n",
    "\n",
    "from hmmlearn.hmm import GaussianHMM, MultinomialHMM\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist.train_images()\n",
    "y_train = mnist.train_labels()\n",
    "X_test = mnist.test_images()\n",
    "y_test = mnist.test_labels()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Normalize the images.\n",
    "X_train = (X_train / 255) - 0.5\n",
    "X_test = (X_test / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "X_train = X_train.reshape((-1, 784))\n",
    "X_test = X_test.reshape((-1, 784))\n",
    "\n",
    "print(X_train.shape) # (60000, 784)\n",
    "print(X_test.shape)  # (10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying a learning algorithm...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=4,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n",
      "\n",
      "Evaluating results...\n",
      "\n",
      "Precision: \t 0.9682\n",
      "Recall: \t 0.9682\n",
      "F1 score: \t 0.9682\n",
      "Accuracy: \t 0.9682\n"
     ]
    }
   ],
   "source": [
    "# Apply a learning algorithm\n",
    "print(\"Applying a learning algorithm...\\n\")\n",
    "clf = RandomForestClassifier(n_estimators=50,n_jobs=4)\n",
    "_ = clf.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction\n",
    "print(\"Making predictions...\\n\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the prediction\n",
    "print(\"Evaluating results...\\n\")\n",
    "print(\"Precision: \\t\", metrics.precision_score(y_test, y_pred, average='micro'))\n",
    "print(\"Recall: \\t\", metrics.recall_score(y_test, y_pred, average='micro'))\n",
    "print(\"F1 score: \\t\", metrics.f1_score(y_test, y_pred, average='micro'))\n",
    "print(\"Accuracy: \\t\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load\n",
    "# with open(\"model 20k-20iter.pkl\", \"rb\") as file: model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Run \n",
    "model = GaussianHMM(n_components=10, covariance_type=\"full\", n_iter = 10)\n",
    "model.fit(X_train[:50000]) # max: 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = model.sample(15, random_state = None) #random_state/seed is inherited\n",
    "print(data[0][0].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(15)\n",
    "point = data[0][idx]# X_train[0]\n",
    "idx\n",
    "plt.imshow(point.reshape((28, 28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [] \n",
    "for i in range(10):\n",
    "    pred = model.predict(X_test[y_test == i])\n",
    "    true_class = Counter(pred).most_common(3)\n",
    "    print(f'Model class: {i} - most common elem(s): {true_class}.\\t  Our models {i}, is considered a {true_class[0][0]}')\n",
    "    ls.append([true_class[0][0], true_class[1][0], true_class[2][0]])\n",
    "\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = deepcopy(ls)\n",
    "assume = [i.pop(0) for i in ls]  \n",
    "second_choice = [i.pop(0) for i in ls]   \n",
    "third_choice = [i.pop(0) for i in ls]\n",
    "\n",
    "    \n",
    "print(assume)\n",
    "print(second_choice)\n",
    "print(third_choice)\n",
    "\n",
    "print(rankings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual changes\n",
    "# ls = [8, 7, 5, 0, 4, 9, 3, 1, 8, 2]\n",
    "\n",
    "def evaluate(mapping, label):\n",
    "\n",
    "    score = 0\n",
    "    points = np.where(y_test==label)[0]\n",
    "\n",
    "    predictions = model.predict(X_test[points])\n",
    "#     labels = y_test[points] # labels is our new y_test\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        if(idx ==0):\n",
    "            print(f'HMMs pred is {pred}. mapping[pred] is {mapping[pred]}, check if = {label}?')\n",
    "        if(mapping[pred] == label):\n",
    "            score += 1\n",
    "    \n",
    "    acc = score/len(points) * 100\n",
    "    print(f'Mapping: {mapping}')\n",
    "    print(f'Accuracy with our mapping: {acc:.2f}%')\n",
    "    return acc\n",
    "\n",
    "print(assume)\n",
    "\n",
    "for i in range (10): \n",
    "    \n",
    "    #select choices\n",
    "    sec = deepcopy(assume)\n",
    "    sec[i] = second_choice[i]\n",
    "\n",
    "    thir = deepcopy(assume)\n",
    "    thir[i] = third_choice[i]\n",
    "    \n",
    "    # compute & compare\n",
    "    label = assume[i]\n",
    "    best = np.argmax([evaluate(assume, label = label), evaluate(sec, label = label), evaluate(thir, label = label)])\n",
    "#     best = np.argmax([evaluate(assume, label = i), evaluate(sec, label = i), evaluate(thir, label = i)]) # first attempt\n",
    "\n",
    "    #update\n",
    "    assume[i] = rankings[i][best]\n",
    "assume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume = \n",
    "score = 0\n",
    "predictions = model.predict(X_test)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if(assume[pred] == y_test[idx]):\n",
    "        score += 1\n",
    "\n",
    "print(f'Accuracy with our mapping: {score/len(predictions) * 100:.2f}%')\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "common = stats.mode(pred)[1][0]/pred.shape[0]\n",
    "print(f'What if we called everyting our most common class: {common*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "pickle.dump(model, open(\"model 50k-10iter.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# with open(\"model.pkl\", \"rb\") as file: pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
